<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8"/>
    <title>Docker</title>
    <link href="/study/index.css" rel="stylesheet"/>
  </head>
  <body>
    <h1>Docker</h1>
    <ul>
      <li><a href="https://gist.github.com/ju2wheels/3d1a1dfa498977874d03">Docker file reference</a></li>
      <li><a href="http://stackoverflow.com/questions/27912917/how-to-configure-docker-port-mapping-to-use-nginx-as-an-upstream-proxy">Docker ports served via nginx</a></li>
      <li><a href="http://stackoverflow.com/questions/22111060/difference-between-expose-and-publish-in-docker">Difference between expose and publish</a> [Basically expose is for inter container communication where as publish allows for communication outside docker containers]</li>
      <li><a href="http://jasonwilder.com/blog/2014/03/25/automated-nginx-reverse-proxy-for-docker/">Automated nginx reverse proxy for docker</a></li>
      <li><a href="http://stackoverflow.com/questions/26153686/how-to-run-a-command-on-an-already-existing-docker-container">Running Command on an existing docker container</a></li>
      <li><a href="http://stackoverflow.com/questions/24706708/how-to-start-docker-container-as-server">Running docker container as a server</a></li>
      <li id="events"><a href="https://docs.docker.com/engine/reference/commandline/events/">A guide to docker events</a> [Docker events can be formatted as json and handled by any program that can read the output from the shell]</li>
      <li><a href="https://docs.docker.com/engine/api/v1.26/">It seems that docker has an http api</a> [Oh yes baby]</li>
      <li><a href="http://stackoverflow.com/questions/24702233/docker-container-and-memory-consumption">The memory consumption of a docker container</a></li>
      <li><a href="http://stackoverflow.com/documentation/docker/3935/docker-remote-api#t=201703080728266851921">Enable remote api for docker</a></li>
      <li><a href="http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker">Persistent storage in docker</a></li>
      <li><a href="https://github.com/tianon/dockerfiles">Big list of dockerfiles</a> [Can help in understanding various commands]</li>
      <li><a href="https://groups.google.com/forum/#!topic/docker-user/RDpH2OYs6Xw">Docker images don't need to contain an os</a></li>
      <li>Docker files can be <a href="https://github.com/docker/docker/issues/3378">chained</a>. [We'll probably have to use this technique to configure our node and redis environments before launching nodebb it seems]</li>
      <li>Every instruction is a layer in a docker image. Which ties well with docker's  <a href="#prop_changes"> change propagation mechanism</a>. </li>
    </ul>
    <h1>Things that we need to do with docker</h1>
    <ol>        
      <li>Perform housekeeping tasks-- dynamically replace the files in the <em>/install/data/</em> directory depending upon <a href="/study/forums.html#stat_data">the type of forum</a> software that we want to create.</li>
      <li>Auto install nodebb with all of it's dependencies in separate (individual) containers -- imagemagick, node, node_modules,redis and nodebb <a href="/study/forums.html#plug_list">plugins</a>.</li>
      <li>The status of installation should be <a href="#events">notified</a> to the process that initiated the installation.</li>
      <li>Configure docker in a way that we are able to <a href="#alter_exec">send commands</a> to it during run time.</li>
    </ol>
    <h1>Notes</h1>
    <p id="prop_changes"><em>"When you change a Docker image, such as when you update an application to a new version, a new layer is built and replaces only the layer it updates. The other layers remain intact. To distribute the update, you only need to transfer the updated layer. Layering speeds up distribution of Docker images. Docker determines which layers need to be updated at runtime."</em>-- <s>This means that we should be able to install plugins and update dependencies on a single docker image and the changes will propagate to all the containers. [Man this is getting better and better]</s> [It seems that the layering only applied to image builds and does not actually propagate to containers. But we have a <a href="#alter_exec">few alternatives</a>]</p>
    <p id="alter_exec">There is an api endpoint that <a href="https://docs.docker.com/engine/api/v1.26/#operation/ContainerExec">creates exec instances</a>. Although we can run command inside a docker container using this api we won't need to do so. Since the docker containers will be networked we can simply instruct them to run a <a href="#prop_updates">command via http</a>. </p>

    <p><em>"Not at all! Any data that your application writes to disk gets preserved in its container until you explicitly delete the container. The file system for the container persists even after the container halts."</em>[It seems that data does persist inside the <a href="http://stackoverflow.com/questions/40850077/why-are-my-non-volume-data-in-docker-container-persistence-after-restarting-the">docker container</a>, <a href="#ex_im">therefore we can devise a strategy</a> to back up the data]</p>
    <p id="ex_im">Docker containers can be compressed into a tar file and then exported, later on the data from the tar file can be imported in a different container. So that whatever data is stored inside the UFS is safe as long as the container is not deleted. [Forget about all the best practices of having a volume seperate from the data and other mumbo jumbo]. This answer gives the details of some alternative <a href="http://stackoverflow.com/questions/26331651/how-can-i-backup-a-docker-container-with-its-data-volumes">data backup</a> strategies.</p>
    <p id="h_name">Docker allows us to <a href="https://docs.docker.com/engine/api/v1.26/#operation/ContainerCreate">set hostnames</a> that can be used to access containers. These hostnames can be randomly generated ids stored in a database  proxied by a master nginx server.</p>
    <p>Docker runs as root by default.</p>
    <p>We can run docker images either <a href="http://stackoverflow.com/questions/18497688/run-a-docker-image-as-a-container">by name or by ids</a>. The advantage of using id is that we won't have to waste any time in laying down a naming convention for our images. The disadvantage is that the id of the image does not give us much hint of what the image actually contains. But the disadvantage is bearable. We won't be changing images. We'll make containers smart instead.</p>
    <p><strong>Propagating updates to docker containers via a git branch</strong></p>
    <div id="prop_updates"><s>
<p>      The problem of propagating updates to the containers can be easily solved by pulling the changes from a git branch. We need to create a base image that initializes a container and the applications in it with to a default state. The actual state of the application would be maintained inside a git repository. Once we push updates to the repo we can instruct the master process to command a container to pull all the updates from the repo and restart the applications. This would work because:-</p>
      <ol>
          <li>The containers are networked</li>
          <li>The master process has the ids of all the containers in a network</li>
      </ol>
      <p>Once the update has been applied to the container and the application server has been restarted we can send a notification to the master process. In openresty terms this simply translates to a handler on the master server that listens to the incoming post requests.</p></s> [Say <a href="#dock_swarm">hello to docker swarm</a>]
    </div>
    <div id="dock_swarm">
      <h1>Hello docker swarm</h1>
      <p>The key difference between networked containers and swarm mode is highlighted in this quote from the official docker documentation</p>
      <p><em>When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.
      </em></p>
      <p><strong>Tasks and services</strong></p>
      <p>A task is a container running on a swarm. A service is a collection of tasks on a swarm.</p>
      <ol>
        <li>A docker node is an instance of docker engine running on a swarm</li>
	<li>The manager node has all the capabilities of a worker node.</li>
        <li>Docker swarm introduces a concept of overlay networks. All the container in an overlay network can communicate with each other regardless of the machine they are hosted on/ the node on which they are deployed on.</li>
        <li>Overlay networks can be created with the help of a api call to the network endpoint with the <a href="https://docs.docker.com/engine/api/v1.26/#operation/NetworkCreate">driver specified as overlay</a></li>
        <li>In the swarm mode if a service is not running on the node, the swarm manager is intelligent enough to route the request to a node on which the service is actually running a phenomenon known as "ingress load balancing".</li>
      </ol>
      <p><strong>Rationale behind using a swarm instead of containers</strong></p>
      <p>We'll its a very simple decision. A docker swarm gives us service level networking with the master process out of te box. With containers we'll have to write our networking logic. I don't want to do that. Yes we're not using the swarm in a way it was intended(to distribute/orchestrate services) but it works well for what we need so it's all good :)</p>
      <p><strong>How are we going to use docker swarm ?</strong></p>
      <p>Here's how we'll structure our application:-</p>
      <ol>
        <li>We'll create a single machine swarm. That is we'll only have one node of execution. We're not considering scalability/distribution/redudance of individual services. Our goal is to run a set of services for a client that are independent from the services of other clients.</li>
        <li>Services will be grouped by an overlay network. An overlay network is the bucket underwich various services will reside. For example a network called "loco" could hold the services for an organization called loco.</li>
	        <li>These services are just containers created from static docker images. If we modify the images propagating the updates to the services is quite straightforward by the means of http api.</li>
        <li>We'll need to hold the service information in a master database. What kind of information? Probably just a has that maps the service metadata with network overlays so that we can access them using our nginx configuration and update them using the docker api. We'll be using a redis instance because the data storage requirements are quite low.</li>

         <li>It should be very much possible to move the docker service to its own machine and then add the node to the swarm so that it's still available on the overlay network. We'll have to update the information in the database if we do that. However we'll try to keep everything running on a single machine and apply vertical scaling to it. If we foresee running out of scale then we'll consider moving out to different machines. The upgrade is quite simple from an operational perspective. We'll replicate the service on a different machine, update the database information and delete the service on the original machine. In essence we'll never be distributing the individual service. We'll only be configuring them in a way so that multiple services can run along side each other efficiently.</li>
      </ol>
<p>On docker's website there is a little experiment that attaches two distinct services to a network overlay. One is an nginx server listening on port 80 and other is a "busy box application". It is shown that from the busy box container it is possible to wget nginx using just the network overlay name i.e without publishing the port outside of the swarm. This is the behaviour that we want to replicate.</p>
    </div>
    <div id="docker_machine">
      <h1>Docker machine</h1>
      <p>Docker machines allow you to create and manage docker hosts. This is just fancy talk for a virtual machine with docker installed on it. Essentially a docker machine is a tool that allows you to remotely log in to a vm and execute docker related commands on it. To the developer it gives the illusion that he is only interacting with docker and pulling the strings with docker machine to manage his remote server.</p>
      <p>I can't find much use for it in the production environments, good old ssh and a setup script is enough for me. Nonetheless its a very handy tool for testing and managing swarms locally. So I'll be using it on my dev machine, where as in production everything will be managed by api calls.</p>
      <p><strong>Some links</strong></p>
      <ul>
          <li><a href="https://rominirani.com/docker-machine-to-control-docker-hosts-on-google-cloud-3a48b46809dc#.w6pna2bv9">Docker machine controlling docker hosts on google cloud</a></li>
      </ul>
    </div>
    <p><small>-- <a href="/about.html">Akshat Jiwan Sharma</a></small></p>     

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-37138783-1']);
      _gaq.push(['_trackPageview']);

      (function ()
      {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    <script src="/jquery-1.11.1.min.js" type="text/javascript"></script>
    <script src="/study/index.js" type="text/javascript"></script>

  </body>
</html>
